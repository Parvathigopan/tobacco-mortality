# -*- coding: utf-8 -*-
"""Tobacco_use_and_mortalty_Project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XqNCryZVKBZst4bi9SA1juGgOzRm-7p3

## Part - I

**Problem Definition**

Objective: The primary goal is to predict the likelihood of mortality based on tobacco use patterns and other relevant factors.

Scope: This involves defining the boundaries of the analysis, which may include focusing on specific demographic or geographic regions. It also emphasizes the importance of considering both direct and indirect factors that can influence mortality rates.
"""

# -*- coding: utf-8 -*-
"""Tobacco Use and Mortality Data Processing Notebook"""
!pip install matplotlib seaborn scikit-learn shap

# Import necessary libraries
import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# Import necessary libraries for modeling
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

# Import necessary libraries for Neural Networks
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Mount Google Drive
from google.colab import drive
drive.mount('/gdrive')

# Define the path to the data directory
path = '/gdrive/My Drive/Tobacco Data/'

"""## Part-II

**Data Collection**

The project outlines several categories of data needed for the analysis:

Health Surveys: Examples given are NHANES and BRFSS. These are large-scale surveys.

Mortality Data: Sources like WHO and CDC are mentioned.

Tobacco Use Data: Information on smoking habits, duration, frequency, and types of tobacco.

Socioeconomic Data: Age, gender, income, education, occupation.

Health Data: Pre-existing conditions, lifestyle, healthcare access.

These are the dataset files used:

*admissions.txt*: This file contains data on hospital admissions, which falls under Health Data. It includes information on diagnoses that can be caused by smoking.

*fatalities.txt*: This file contains data on mortality, so it represents Mortality Data, specifically related to causes that can be linked to smoking.

*metrics.txt*: This file has data on tobacco prices, household expenditure on tobacco, etc., which could be considered Socioeconomic Data related to tobacco consumption.

*prescriptions.txt*: This file provides data on prescriptions for smoking cessation aids, which is related to Tobacco Use Data and Health Data.

*smokers.txt*: This file contains data on smoking prevalence by age and gender, which is key Tobacco Use Data and Socioeconomic Data.
"""

import os
import pandas as pd

path = "/gdrive/My Drive/Tobacco Data/"  # replace with your actual path

data = {file[:-4]: pd.read_csv(path + file) for file in os.listdir(path) if file.endswith('.csv')}

# Print the first 5 rows of each DataFrame
for name, df in data.items():
    print(f"\n{name} Data:")
    print(df.head())

"""## Part - III

**Data Preprocessing**

The project outlines the following key preprocessing steps:

Data Cleaning: Handling missing values, outliers, and inconsistencies.

Data Integration: Merging datasets.

Feature Engineering: Creating new features from existing ones.
"""

# Load datasets
data = {file[:-4]: pd.read_csv(path + file) for file in os.listdir(path) if file.endswith('.csv')}

# Cleaning the Year columns
clean_year = lambda x: x[:4]
data['prescriptions']['Year'] = data['prescriptions']['Year'].apply(clean_year)
data['admissions']['Year'] = data['admissions']['Year'].apply(clean_year)

# Function to get smokers data
def get_smokers(data):
    unpivoted = pd.melt(data['smokers'], id_vars=['Year', 'Sex'],
                        value_vars=['16-24', '25-34', '35-49', '50-59', '60 and Over'],
                        var_name="Age", value_name="Smokers")
    gendered = unpivoted[unpivoted.Sex.notna()]  # Use notna() to filter out NaN values
    metric_cols = ["Year", "Real Households' Disposable Income", "Tobacco Price\nIndex"]
    add_metrics = gendered.merge(data['metrics'][metric_cols], on="Year")

    # Ensure 'Year' is treated as a string for merging
    add_metrics['Year'] = add_metrics['Year'].astype(str)
    data['prescriptions']['Year'] = data['prescriptions']['Year'].astype(str)

    presc_cols = ["Year", 'All Pharmacotherapy Prescriptions', 'Net Ingredient Cost of All Pharmacotherapies']
    add_prescriptions = add_metrics.merge(data['prescriptions'][presc_cols], on="Year", how="left")
    final_cols = ['Year', 'Sex', 'Age', 'Smokers', 'Income', 'Cost', 'In Treatment', 'Drug Cost']
    add_prescriptions.columns = final_cols
    return add_prescriptions

# Get smokers data
se_data = {}
se_data['Smokers'] = get_smokers(data)

# Function to get admissions data
def get_admissions(data):
    df = data['admissions']
    df = df[(df["Sex"].isna()) & (df['Metric'] == "Attributable number")]
    df['Value'] = df['Value'].astype('float')
    grouped = df.groupby(['Year', "Diagnosis Type"])['Value'].sum().reset_index()
    onlycancer = grouped[grouped['Diagnosis Type'].str.endswith("smoking")]
    cancer_admissions = onlycancer.groupby('Year')['Value'].sum().reset_index()
    cancer_admissions.columns = ['Year', 'Admissions']  # Set the correct column names
    cancer_admissions['Year'] = cancer_admissions['Year'].astype(str)  # Ensure Year is string
    return cancer_admissions

# Get admissions data
se_data['Admissions'] = get_admissions(data)

# Function to get deaths data
def get_deaths(data):
    df = data['fatalities']
    df = df[(df["Sex"].isna()) & (df['Metric'] == "Attributable number")]
    df['Value'] = df['Value'].astype('float')
    grouped = df.groupby(['Year', "Diagnosis Type"])['Value'].sum().reset_index()
    onlycancer = grouped[grouped['Diagnosis Type'].str.endswith("smoking")]
    deaths = onlycancer.groupby('Year')['Value'].sum().reset_index()
    deaths.columns = ['Year', 'Deaths']  # Set the correct column names
    deaths['Year'] = deaths['Year'].astype(str)  # Ensure Year is string
    return deaths.merge(se_data['Admissions'], on='Year')

# Get deaths data
se_data['Deaths'] = get_deaths(data)

# Prepare the flattened dataset
metrics_data = data['metrics'][['Year', "Real Households' Disposable Income", "Tobacco Price\nIndex"]]
metrics_data['Year'] = metrics_data['Year'].astype(str)  # Ensure Year is string for merging

flattened = se_data['Smokers'].merge(se_data['Deaths'], on='Year').merge(metrics_data, on='Year')

# Save the flattened dataset
flattened.to_csv("/gdrive/My Drive/Tobacco Data/flattened.csv", index=False)

# Display the first few rows of the flattened dataset
flattened.head()

"""## Part - IV

**Exploratory Data Analysis (EDA)**

● Visualizations: Use histograms, scatter plots, and heatmaps to understand
 distributions and correlations.

 ● Statistical Analysis: Perform correlation analysis to identify significant
 relationships between features and mortality.
"""

# Load and clean data
df = pd.read_csv("/gdrive/My Drive/Tobacco Data/flattened.csv")
df.columns = df.columns.str.strip().str.replace('\n', ' ')
# Encode categorical columns
from sklearn.preprocessing import LabelEncoder
df['Sex'] = LabelEncoder().fit_transform(df['Sex'])
df['Age'] = LabelEncoder().fit_transform(df['Age'])

# EDA - Visualizations
sns.histplot(df['Deaths'], kde=True)
plt.title('Distribution of Deaths')
plt.show()

sns.pairplot(df[['Deaths', 'Smokers', 'Income', 'Drug Cost', 'Admissions']])
plt.suptitle("Scatter Plots Between Key Variables and Deaths", y=1.02)
plt.show()

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Feature Correlation Heatmap')
plt.show()

# Correlation with Deaths
correlation_with_deaths = df.corr()['Deaths'].sort_values(ascending=False)
print("\nCorrelation with Deaths:\n", correlation_with_deaths)

"""## Part - V

**Model Selection**

● Supervised Learning Algorithms: Consider algorithms suitable for classification
 problems such as:

  ○ Logistic Regression.

 ○ Decision Trees and Random Forests.

 ○ Gradient Boosting Machines (e.g., XGBoost, LightGBM).

 ○ Support Vector Machines (SVM).

 ○ Neural Networks
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Dictionary of classification models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "LightGBM": LGBMClassifier(random_state=42),
    "SVM": SVC(probability=True, random_state=42),
    "Neural Network": MLPClassifier(max_iter=500, random_state=42)
}

"""## Part - VI

**Model Training and Evaluation**

 ● Training: Split the dataset into training and testing sets. Use cross-validation to
 ensure robust model performance.

 ● Evaluation Metrics: Choose appropriate metrics such as accuracy, precision,
 recall, F1-score, and ROC-AUC score.

 ● Model Tuning: Perform hyperparameter tuning to optimize model performance
 using techniques like Grid Search or Random Search
"""

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np

# Split dataset
X = df.drop(columns=['Deaths'])
y = (df['Deaths'] > df['Deaths'].median()).astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Function to evaluate models
def evaluate_model(name, model):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else model.decision_function(X_test)

    print(f"\n{name} Evaluation:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1-Score:", f1_score(y_test, y_pred))
    print("ROC-AUC:", roc_auc_score(y_test, y_proba))

    # Cross-validation score (F1)
    cv_score = cross_val_score(model, X, y, cv=cv, scoring='f1')
    print("Cross-Validated F1 Score (mean):", np.mean(cv_score))

# Evaluate all models
for name, model in models.items():
    evaluate_model(name, model)

"""Best model :- Random Forest

Model Tuning
"""

# Define hyperparameter grid
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Grid Search on Random Forest
grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf,
                              cv=cv, scoring='f1', n_jobs=-1, verbose=1)
grid_search_rf.fit(X_train, y_train)

# Best estimator evaluation
best_rf = grid_search_rf.best_estimator_
evaluate_model("Tuned Random Forest", best_rf)

"""## Part - VII

**Model Interpretation**

● Feature Importance: Identify which features are most influential in predicting
 mortality.

 ● Model Explainability: Use tools like SHAP (SHapley Additive exPlanations) or
 LIME (Local Interpretable Model-agnostic Explanations) to explain model
 predictions.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Feature importances from Random Forest or tuned model
importances = pd.Series(best_rf.feature_importances_, index=X.columns)
importances.sort_values().plot(kind='barh', figsize=(10, 6), title="Feature Importances (Random Forest)")
plt.xlabel("Importance Score")
plt.tight_layout()
plt.show()

"""## Part - VIII

**Deployment**

 ● API Development: Create an API for the model using frameworks like Flask or
 FastAPI.

 ● WebApplication: Develop a user interface to input data and display predictions
 using frameworks like Streamlit or Dash.
"""

import joblib
joblib.dump(best_rf, "best_model.pkl")  # Or use best_rf if preferred

import pandas as pd

# Pick one test entry from the dataset (e.g., first row)
sample = X_test.iloc[0]
sample_df = pd.DataFrame([sample])

# Make prediction using best_rf
prediction = best_rf.predict(sample_df)[0]
probability = best_rf.predict_proba(sample_df)[0][int(prediction)]

# Display result
print("🔍 Sample Input Features:")
print(sample.to_string())

print("\n✅ Prediction Result:")
print(f"Predicted Mortality Class: {'High Risk' if prediction else 'Low Risk'}")
print(f"Model Confidence: {probability:.2%}")